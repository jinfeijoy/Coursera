topics:
-basic architecture (fully-connected neural network)
-data preparation (ie. normalize the inputs before fitting your model)
-activation functions (eg. sigmoid vs ReLU etc)
-backpropagation
-learning rate
-learning momentum and ADAM
-batch normalization
-convolutional neural networks
-max pool 

-the activation function
        - consider the derivative of that function to see why ReLU works better than sigmoid
        - ReLU: https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b  
       
- dataset for projects:
        - http://yann.lecun.com/exdb/mnist/ 
        - https://www.cs.toronto.edu/~kriz/cifar.html 
        - https://www.kaggle.com/datasets
        - http://archive.ics.uci.edu/ml/index.php
        
        
- some paper:
        - https://arxiv.org/abs/1711.09846 
        - https://arxiv.org/abs/1711.06402
