SVM:
  -Trade-off on bias and variance:
    -C(-1/lembda): Large C: lower bias, high variance.
                   Small C: higher bias, lower variance
     -sigma^2: Large Sigma^2: Features fi vary more smothly, higher bias, lower variance
               Small Sigma^2: Features fi vary less smoothly, lower bias, higher variance
  -Packages: liblinear, libsvm, 
      -choice of parameter C
      -Choice of kernel (similarity function):
          -eg. no kernel ("linear kernel") predict y=1 if theta'*x>=0 (if n is large, m is small, huge number features and small dataset, use linear kernel to avoid overfitting)
          -eg. gaussian kernel: need to choose sigma^2 (if n is small, and m is large)
                - sometimes it requires to write the kernel functions: kernel(x1,x2)=exp( -1 * (||x1 - x2|| ^ 2)/ (2 * sigma ^ 2))
                - ||x-l||^2=(x1-l1)^2+(x2-l2)^2+(x3-l3)^2+...+(xn-ln)^2
                - when write the kernel function, if the features are in different scales, remember to perform feature scaling before using the gaussian kernel.
       -Other choice of kernel
           -Condition: not all similarity functions similarity(x,l) make valid kernels, need to satisfy echnical condition called "mercer's theorem" to make sure SVM packages' optimizations run correctly, and do not diverge.
           - Other kernels: -polynomial kernel: k(x,l)=(X'l+constant)^degree: x and l are all negative or x and l are very similar
                            -more esoteric (not popular): string kernel (data are text or other text strings), chi-square kernel, histogram intersection kernel, ...
        -multi-class classification: y = {1,2,...,K}:
            -one vs all method: train K SVMs, one to distinguish y=i from the rest, for i=1,2,...,k, get theta(1), theta(2),...,theta(K), pick class i with largest (theta(i))'x
  -Logistic regression vs SVMs:
        - if n=number of features, m = number of training examples
        - if n is large (relative to m, n>=m, n=10,000, m=10~1000), then use logistic regression, or SVM without a kernel ("linear kernel")
        - if n is small, m is intermediate: use SVM with gaussian kernel (n=1~1000, m=10~10,000)
        - if n is small, m is large (n=1~1000, m=50,000+): create/add more features, then use logistic regression or SVM without a kernel
        - neural network likely to work well for most of these settings, but may be slower to train.
        - local minimal is not a problem for SVM, but sometimes, it is a problem for neural network
        
        
        
Unsupervised Learning: 
	- Clustering:
		- K-means: the most popular algorithm
			1. randomly initialize K cluster centroids u1, u2, ... uK
			2. Repeat:
				1) for i =1 to m, c(i) := index (from 1 to K) of cluster centroid closedst to x(i)
				2) for k = 1 to K, uk := average (mean) of points assigned to cluster k

	
	

        
