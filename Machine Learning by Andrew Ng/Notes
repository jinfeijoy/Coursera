SVM:
  -Trade-off on bias and variance:
    -C(-1/lembda): Large C: lower bias, high variance.
                   Small C: higher bias, lower variance
     -sigma^2: Large Sigma^2: Features fi vary more smothly, higher bias, lower variance
               Small Sigma^2: Features fi vary less smoothly, lower bias, higher variance
  -Packages: liblinear, libsvm, 
      -choice of parameter C
      -Choice of kernel (similarity function):
          -eg. no kernel ("linear kernel") predict y=1 if theta'*x>=0 (if n is large, m is small, huge number features and small dataset, use linear kernel to avoid overfitting)
          -eg. gaussian kernel: need to choose sigma^2 (if n is small, and m is large)
                - sometimes it requires to write the kernel functions: kernel(x1,x2)=exp( -1 * (||x1 - x2|| ^ 2)/ (2 * sigma ^ 2))
                - ||x-l||^2=(x1-l1)^2+(x2-l2)^2+(x3-l3)^2+...+(xn-ln)^2
                - when write the kernel function, if the features are in different scales, remember to perform feature scaling before using the gaussian kernel.
       -Other choice of kernel
           -Condition: not all similarity functions similarity(x,l) make valid kernels, need to satisfy echnical condition called "mercer's theorem" to make sure SVM packages' optimizations run correctly, and do not diverge.
           - Other kernels: -polynomial kernel: k(x,l)=(X'l+constant)^degree: x and l are all negative or x and l are very similar
                            -more esoteric (not popular): string kernel (data are text or other text strings), chi-square kernel, histogram intersection kernel, ...
        -multi-class classification: y = {1,2,...,K}:
            -one vs all method: train K SVMs, one to distinguish y=i from the rest, for i=1,2,...,k, get theta(1), theta(2),...,theta(K), pick class i with largest (theta(i))'x
  -Logistic regression vs SVMs:
        - if n=number of features, m = number of training examples
        - if n is large (relative to m, n>=m, n=10,000, m=10~1000), then use logistic regression, or SVM without a kernel ("linear kernel")
        - if n is small, m is intermediate: use SVM with gaussian kernel (n=1~1000, m=10~10,000)
        - if n is small, m is large (n=1~1000, m=50,000+): create/add more features, then use logistic regression or SVM without a kernel
        - neural network likely to work well for most of these settings, but may be slower to train.
        - local minimal is not a problem for SVM, but sometimes, it is a problem for neural network
        
        
        
Unsupervised Learning: 
	- Clustering:
		- K-means: the most popular algorithm
			1. randomly initialize K cluster centroids u1, u2, ... uK
			2. Repeat:
				1) for i =1 to m, c(i) := index (from 1 to K) of cluster centroid closedst to x(i)
				2) for k = 1 to K, uk := average (mean) of points assigned to cluster k
		- K-means for non-separated clusters: use for market segmentation
		- Optimization Objective (k-means): to minimize the sum of xi and centroid k distance.
		- Random Initialization: K < m, randomly pick K training examples, set mu1, mu2, ..., muK equal to these K examples.
			- Some problem: it may cause local optima
				- to solve this problem, try multiple random initilization: 
					1. for i = 1 : 100 { randomly initialize K-means, run k-means, compute cost function}
					2. pick clustering that gave lowest cost J
		- Choose the number of clusters:
			- choose the number of clusters by hand based on the visualization.
			- Elbow method: run multiple no.of.clusters and get the cost function J, select the trade off no.of.clusters and cost function value. 
			- evaluate k-means based on a metric for how well it performs for that later purpose.
	- PCA (Principal Component Analysis):
		- try to find a surface to minimize the sum of square distance (projection error)
			- reduce from n-dimension to k-dimension: find k vectors u(1), u(2), ..., u(k) onto which to project the data, so as to minimize the projection error.
			- PCA is not linear regression, all x are treated equally, nothing special, no predicted y here.
		- feature scaling and nomorlization: data preprocessing (feature scaling / mean normalization)
		- Summary:
			- after mean normalization (ensure every feature has zero mean) and optionally feature scalling: 
				-Sigma = 1/m * Sum(xi * xi');
				- [U,S,V]=svd(Sigma);
				- Ureduce = U(:, 1:k);
				- z =  Ureduce' * x;
		- Reconstruction from compressed representation: z = Ureduce'*x, thus, x=Ureduce*z, Ureduce is n*k matrix, x is n*n matrix
		- choosing the number of PC (k): 
			- average squared projection error: 1/m*sum||xi-x(approx)i||^2
			- total variation in the data: 1/m * sum||xi||^2
			-

	
	

        
