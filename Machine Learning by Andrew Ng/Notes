SVM:
  -Trade-off on bias and variance:
    -C(-1/lembda): Large C: lower bias, high variance.
                   Small C: higher bias, lower variance
     -sigma^2: Large Sigma^2: Features fi vary more smothly, higher bias, lower variance
               Small Sigma^2: Features fi vary less smoothly, lower bias, higher variance
  -Packages: liblinear, libsvm, 
      -choice of parameter C
      -Choice of kernel (similarity function):
          -eg. no kernel ("linear kernel") predict y=1 if theta'*x>=0 (if n is large, m is small, huge number features and small dataset, use linear kernel to avoid overfitting)
          -eg. gaussian kernel: need to choose sigma^2 (if n is small, and m is large)
                - sometimes it requires to write the kernel functions: kernel(x1,x2)=exp( -1 * (||x1 - x2|| ^ 2)/ (2 * sigma ^ 2))
                - ||x-l||^2=(x1-l1)^2+(x2-l2)^2+(x3-l3)^2+...+(xn-ln)^2
                - when write the kernel function, if the features are in different scales, remember to perform feature scaling before using the gaussian kernel.
       -Other choice of kernel
           -Condition: not all similarity functions similarity(x,l) make valid kernels, need to satisfy echnical condition called "mercer's theorem" to make sure SVM packages' optimizations run correctly, and do not diverge.
           - Other kernels: -polynomial kernel: k(x,l)=(X'l+constant)^degree: x and l are all negative or x and l are very similar
                            -more esoteric (not popular): string kernel (data are text or other text strings), chi-square kernel, histogram intersection kernel, ...
        -multi-class classification: y = {1,2,...,K}:
            -one vs all method: train K SVMs, one to distinguish y=i from the rest, for i=1,2,...,k, get theta(1), theta(2),...,theta(K), pick class i with largest (theta(i))'x
  -Logistic regression vs SVMs:
        - if n=number of features, m = number of training examples
        - if n is large (relative to m, n>=m, n=10,000, m=10~1000), then use logistic regression, or SVM without a kernel ("linear kernel")
        - if n is small, m is intermediate: use SVM with gaussian kernel (n=1~1000, m=10~10,000)
        - if n is small, m is large (n=1~1000, m=50,000+): create/add more features, then use logistic regression or SVM without a kernel
        - neural network likely to work well for most of these settings, but may be slower to train.
        - local minimal is not a problem for SVM, but sometimes, it is a problem for neural network
        
        
        
Unsupervised Learning: 
	- Clustering:
		- K-means: the most popular algorithm
			1. randomly initialize K cluster centroids u1, u2, ... uK
			2. Repeat:
				1) for i =1 to m, c(i) := index (from 1 to K) of cluster centroid closedst to x(i)
				2) for k = 1 to K, uk := average (mean) of points assigned to cluster k
		- K-means for non-separated clusters: use for market segmentation
		- Optimization Objective (k-means): to minimize the sum of xi and centroid k distance.
		- Random Initialization: K < m, randomly pick K training examples, set mu1, mu2, ..., muK equal to these K examples.
			- Some problem: it may cause local optima
				- to solve this problem, try multiple random initilization: 
					1. for i = 1 : 100 { randomly initialize K-means, run k-means, compute cost function}
					2. pick clustering that gave lowest cost J
		- Choose the number of clusters:
			- choose the number of clusters by hand based on the visualization.
			- Elbow method: run multiple no.of.clusters and get the cost function J, select the trade off no.of.clusters and cost function value. 
			- evaluate k-means based on a metric for how well it performs for that later purpose.
	- PCA (Principal Component Analysis):
		- try to find a surface to minimize the sum of square distance (projection error)
			- reduce from n-dimension to k-dimension: find k vectors u(1), u(2), ..., u(k) onto which to project the data, so as to minimize the projection error.
			- PCA is not linear regression, all x are treated equally, nothing special, no predicted y here.
		- feature scaling and nomorlization: data preprocessing (feature scaling / mean normalization)
		- Summary:
			- after mean normalization (ensure every feature has zero mean) and optionally feature scalling: 
				-Sigma = 1/m * Sum(xi * xi');
				- [U,S,V]=svd(Sigma);
				- Ureduce = U(:, 1:k);
				- z =  Ureduce' * x;
		- Reconstruction from compressed representation: z = Ureduce'*x, thus, x=Ureduce*z, Ureduce is n*k matrix, x is n*n matrix
		- choosing the number of PC (k): 
			- average squared projection error: 1/m*sum||xi-x(approx)i||^2
			- total variation in the data: 1/m * sum||xi||^2
			- averate_squared_projection_error/total_variation_in_the_data <=0.01: 99% of variance is retained
			- try different k, then to see if the value <= 0.01
			- using function [U,S,V]=svd(Sigma) can make it simple: to calculate 1-SUMk(Sii)/SUMn(Sii).
		- Bad use of PCA: To prevent overfitting. this might work ok, but isn't a good way to address overfitting. Use regularization instead.
		- Before implementing PCA, first try running whatever you want to do with the original/raw data x(i), only if that doesn't do what you want, then implement PCA and consider using z(i).
		- to visulize data, to reduce runing memory, to speed up the model
		
Anomaly Detection: 
	- Some Examples:
		- Fraud Detection: 
			- x(i) = features of user i's activities, Model p(x) from data, identify unusual users by checking which have p(x) < ebsilo
		- Manufacturing: 
		- Monitoring computers in a data center:
			- x(i) = features of machinesi, eg. x1=memory use, x2=number of disk accesses, x3=CPI load, x4=CPU load/network traffic
			- p(x) < ebsilo
	- Algorithm: 
		1. choose features xi that you think might be indicative of anomalous examples
		2. fit parameters mu1, ..., mun, sigma1^2, .., sigman^2, muj=mean(xj), sigmaj^2=mean((xj-muj)^2)
		3. given new example x, compute p(x): p(x)= SIGMA(1/(sqrt(2*PAI)*sigmaj)*exp(-1*(xj-muj)^2/(2*sigmaj^2))), anomaly if p(x)<ebsilo
	- Building an anomaly detection system:

		- If we know 10000 good, 20 bad, then 
			- training set: 6000 good
			- CV: 2000 good and 10 bad
			- test: 2000 good and 10 bad
		- reak-number evaluation: 
			- predict 1/0 on CV and test dataset, 
			- y = 1 if p(x) < ebsilo (anomaly), otherwise, y=0 (normal)
			- possible evaluation matrix: TP, FP, FN, TN; Precision/Recall; F1-score
		- anomaly detection vs supervised learning
			- Anomaly detection:
				- very small number of positive examples (y=1): (0-20 is common)
				- Large number of negative (y=0) examples
				- many different types of anomalies, hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we've seen so far.
			- Supervised Learning:
				- large number of positive and negative examples
				- enough positive examples for algorithm to get a sense of what positive examples are like, future positive examples likely to be similar to ones in training set.
		- Application
			- Anomaly detection:
				- fraud detaction
				- manufacturing (e.g. aircraft engines)
				- monitoring machines in a data center, etc.
			- supervised learning:
				- email spam classification
				- weather prediction
				- cancer classification, etc.
		- Feature selection
			- plot the data to histogram plot, to see if the distribution is gaussian distribution, for those non-gaussian distribution variable, do some transformation, like log(x), log(x+c), sqrt(x), x^(1/3), etc.
			- error analysis:
				- p(x) is comparable (say, both large) for normal and anomalous examples: need to create new feature
			- choose features:
				- choose features that might take on unusually large or small values in the event of an anomaly
				- eg. monitoring computers in a data center:
					- x1 = memory use of computer, x2 = number of disk accesses/sec, x3 = CPU load, x4 = network traffic, x5 = (CPI load)^2/network traffic
	- Multivariate Gaussian distribution:
		- instead of doing model p1, p2, etc. separately, model p(x) all in one go. parameters: mu belongs to R, Sigma belongs to Rn*n (covariance matrix)
		- p(x; mu, Sigma) = 1/((2*PAI)^(n/2)*|Sigma|^(1/2))*exp(-1/2*(x-mu)'*Sigma^-1*(x-mu))
			1. fit model p(x) by setting: mu = mean(xi), Sigma=mean((xi-mu)*(xi-mu)')
			2. given a new example x, compute p(x) using the above fomula, flag an anomaly if p(x) < ebsilo.
		- Original Model
			- more often, manually create features to capture anomalies where x1, x2 take unusual combinations of values
			- computationally cheaper (alternatively, scales better to large n)
			- OK even if m (training set size) is small
		- Multivariate Gaussian Model
			- automatically captures correlations between features
			- computationally more expensive
			- must have m>n (number of examples > number of features) (m>=10n would be a reasonable rule), or else Sigma is non-invertible.
			- debug: if Sigma is non-invertible, then check redundant features, to check if x1=x2, and x1=x2+x3, etc
Recommender Systems: 	

        
