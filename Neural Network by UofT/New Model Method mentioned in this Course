- Lesson 4:
  - t-SNE: t-Distributed Stochastic Neighbor Embedding (t-SNE) is a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. 

- Lesson 6: Optimization: How to make the learning go faster
  - Momentum Method:
          - it damps oscillations in directions of high curvature by combining gradients with opposite signs.
          - it builds up speed in directions with a gentle but consistent gradient.
  - The equation of Momentum Method: can be found in the neural network slides: https://d3c33hcgiwev3.cloudfront.net/_4bd9216688e0605b8e05f5533577b3b8_lec6.pdf?Expires=1509840000&Signature=kAjFj-pYSO~AL032-X7dFW5ABisiYaQqQ86FiDYV7KNL-hKXqCO1YfZ6SUiL4MIaXl06qIzjhmHdpRnhJCjRH2vj-9zn5M196m1RAOeSHLItez8jixmBnFoOOXW4OJ4tGdSxxw2AqCkXqWGx~QqrVG~eJ-CK7PzY1KNxLWceb2M_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A 
  - momentum and learning rate: 
            use small learning rate and a big momentum, allows you to get the overall learning rate, the learning rate would be much larger than only use the small learning rate. 
            if only use the big learning rate by itself, the it would cause divergent oscillations.
  - A better type of momentum:
          - the standard momentum method: 
                - first, compute the gradient at the current location and 
                - then take a big jump in the direction of the updated accumulated gradiant
          - Ilya Sutskever (inspired by the Nesterov method for optimizing convex function): 
                - first, make a big jump in the direction of the previous accumulated gradiant
                - then measure the gradient where you end up and make a correction   
  - Individual Learning Rate: 
          - start with a local gain of 1 for every weight
          - increase the local gain if the gradient for that weight does not change sign
          - use small additive increases and multiplicative decreases.
              - this ensures that big gains decay rapidly when oscillations start
              - if the gradient is totally random the gain will hover around 1 when we increase by plus delta half the time and decrease the times 1-delta half the time.
              - the function can be found in the slides
 - rmsprop: divide the gradient by a running average of its recent magnitude.
          - it is a basic ways to learning large neural network with a large redundant dataset
          - rprop: using only the sign of the gradient, full-batch method, does not work with mini-batches
                  - this combines the idea of only using the sign of the gradient with the idea of adapting the step size separately for each weight.
                  - increase the step size for a weight multiplicatively (eg. times 1.2) if the signs of its last two gradients agree.
                  - otherwise, decrease the step size for a weight multiplicatively (eg. times 0.5) 
                  - limit the step sizes to be less than 50 and more than a millionth (Mike Shuster's advice).
          -rmsprop: a mini-batch version of rprop
                  - rprop is equivalent to using the gradient but also dividing by the size of the gradient
                        - the problem with mini-batch rprop is that we divide by a different number for each mini-batch, so why not force the number we divide by to be very similar for adjacent mini-batches?
                  - rmsprop: keep a moving average of the squared gradient for each weight
     - Summary:
          - For small datasets: (e.g. 10,000 cases) or bigger datasets without too much redundancy, use a full-batch method
                - Conjugate gradient, LBFGS ...
                - adaptive learning rates, rprop ...
          - For big, redundant datasets use mini-batches.
                - Try gradient descent with momentum.
                - Try rmsprop (with momentum ?)
                - Try LeCun's latest recipe
          - Why there is no simple recipe:
                - neural nets differ a lot
                    - very deep nets (especially ones with narrow bottlenecks)
                    - recurrent nets
                    - wide shallow nets
                - task differ a lot:
                    - some require very accurate weights, some dont
                    - some have many very rare cases (e.g. words)


Lecture 7a. Modelling Sequences: A brief overview:
Targets: to show how the recursive nueral network are related to the models vectors.

- Memoryless model for sequences: autoregression models
- Beyond memoryless models: give the generative model some hidden state, and give hidden state its own internal dynamics,
- How rnn differ from all other standard models:
  - Linear Dynamical System (engineers love them!)
  - Hidden Markov Models (computer scientists love them!) 
      - fundamental limitation of HMMs: N hidden states it can only remember log(N) bits about what it generated so far.
      - stochastic models
  - RNN:very powerful, because they combine two properties:
      - Distributed hidden state what allows them to store a lot of information about the past efficiently
      - Non-linear dynamics that allows them to update their hidden state in complicated ways.
      - with enough neurons and time, RNNs can compute anything that can be computed by your computer.
      - what kinds of behaviour can RNNs exhibit?
          - they can oscillate. good for motor control?
          - they can settle to point attractors. good for retrieving memories?
          - they can behave chaotically. bad for information processing?
          - RNNs could potentially learn to implement lots of small programs that each capture a nugget of knowledge and run in parallel, interating to produce very complicated effects.
      - But the computational power of RNNs makes them very hard to train.
      - deterministic
      
  - Feedforward net: 
      - we must decide in advance the maximum number of digits in each number.
      - feedforward nets do not generalize well on the binary addtion tast.
- Why it is hard to train RNNs
   - the backward pass is linear:
      - there is a big difference between the forward and backward passes:
        - in the forward pass we use squashing functions (like the logistic) to prevent the activity vectors from exploding.
        - the backward pass, is completely linear. if you double the error derivatives at the final layer, all the error derivatives will double.
   - the problem of exploding or vanishing gradients:
      - if the weights are small, the gradients shrink exponentially, if the weights are big the gradients grow exponentially.
      - typical feed-forward neural nets can cope with these exponential effects because they only have a few hidden layers.
      - in an RNN trained on long sequences ( e.g. 100 time steps) the gradients can easily explode or vanish.
        - we can avoid this by initializing the weights very carfelly.
      - even with good initial weights, its very hard to detect that current target output depends on an input from many time-steps ago. 
        - so RNNs have difficulty dealing with long-range dependencies.
      - Four effective ways to learn an RNN:
        - long short term memory
            - Succeussful on recognizing hand writing.
            - Hochreiter & Schmidhuber, they designed a memory cell using logistic and linear units with multiplicative interactions, 
              information gets into the cell whenever its 'write' gate is on. the information stays in the cell so long as its 'keep' gate is on.
              information can be read from the cell by turning on its 'read' gate.
            
        
Lecture 8. 
  -
        - Hessian Free Optimization
        - Echo State Neuworks
        - Good initialization with momentum
  - best language model so far (upto 2012):
        - Tomas Mikolov and his collaborators have recently trained quite large RNNs on quite large training sets using BPTT.
              -they do better than feed-forward neural nets.
              -they do better than the best other models
              -they do even better when averaged with other models
         -RNNs require much less training data to reach the same level of performance as other models.
         - Rnns improve faster than other methos as the dataset gets bigger: this is going to make them very hard to beat.
  - Echo state networks
  	- Key ideas of echo state networks
		- a very simple way to learn a feedforwrad network is to make the early layers random and fixed.
		- then we just learn the last layer which is a linear model that uses the transformed inputs to predict the target outputs
			- a big random expansion of the input vector can help
		- the equivalent idea for RNNs is to fix the input->hidden connections and the hidden->hidden connections at random values and only learn the hidden->output connections.
			- the learning is then very simple (assuming linear output units)
			- its important to set the random connections very carfully so that the RNN does not explode or die.
	- setting the random connections in an echo state network (details in the slides)
	- good aspect of ESN:
		-can train very fast because they just fit a linear model
		- they demonstrate that its very important to initialize weights sensibly
		- they can do impressive modelling of one-dimensional time-series
			-but they cannot compete seriously for high-dimensional data like pre-processed speech
	- bad aspectes of ESNs
		- they need many more hidden units for a given task then an RNN that learns the hidden->hidden weights
		-Ilya Sutskerver (2012) has shown that if the weights are initilized using the ESN methods, RNNs can be trained very effectively.
		
Lesson 9. Overview of ways to improve generalization
	- Preventing Overfitting:
		1. get more data
			- almost always the best bet if you have enough compute power to train on more data
		2. use a model that has the right capacity
			- enough to fit the true regularities
			- not enough to also fit spurious regularities (if they are weaker)
		3. average many different models
			- use models with different forms
			- or train the model on different subsets of the training data (this is called "bagging")
		4. (Bayesian) use a single neural network architecture, but average the predictions made by many different weight vectors.
	- Limit the capacity of a neural net:
		1. architecture: limit the number of hidden layers and the number of units per layers
		2. early stopping: start with small weights and stop the learning before it overfits
			- when the weights are very small, every hidden unit is in its linear range.
			- as the weights grow, the hidden units start using their non-linear ranges so the capacity grows.
		3. weight-decay: penalize large weights using penalties or constraints on their squared values (L2 penalty) or absolute values (L1 penalty)
			- L2 weight cost:
				- it prevents the network from using weights that it does not need
					- this can often improve generalization a lot because it helps to stop the network from fitting the sampling error
					- it makes a smoother model in which the output changes more slowly as the input changes
				- if the network has two very similar inputs it prefers to put half the weight on each rather than all the weight on one.
			- weight constraints:
				- weight constraints:
					- we usually penalize the squared value of each weight separately, instead, we can put a constraint on the maximum squared length of the incoming weight vector of each unit.
						if an update violates this constraint, we scale down the vector of incoming weights to the allowed length.
					- have several advantages over weight penalties.
						- its easier to set a sensible value
						- they prevent hidden units getting stuck near zero
						- they prevent weights exploding
					- when a unit hits it's limit, the effective weight penalty on all of it's weights is determined by the big gradients.
						- this is more effective than a fixed penalty at pushing irrelevant weights toward zero.
		4. noise: add noise to the weights or the activities
   			- L2 weight-decay via noisy inputs.
				- e.g. add Gaussian noise to the inputs
	- the bayesian interpretation of weight decay:
	- MacKay's quick and dirty method of fixing weight costs:
		- 
   


